{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c790de75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6205ebad-49a9-435a-bf84-ae57d29c2068",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "four_words = ['up', 'data', 'medicine', \"economics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a9bd11-2a14-44f5-8093-b0a696324ce1",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('down', 0.9159865975379944),\n",
       " ('out', 0.8888224363327026),\n",
       " ('back', 0.8425763845443726),\n",
       " ('put', 0.8326882123947144),\n",
       " ('just', 0.8320373892784119),\n",
       " ('off', 0.8262820839881897),\n",
       " ('over', 0.810619056224823),\n",
       " ('away', 0.8059401512145996),\n",
       " ('them', 0.804772675037384),\n",
       " ('into', 0.803209662437439)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "# similar words to the first word \"up\":\n",
    "glove_wiki_vectors.most_similar(four_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('information', 0.7920401096343994),\n",
       " ('analysis', 0.756445586681366),\n",
       " ('tracking', 0.7226751446723938),\n",
       " ('database', 0.7215273976325989),\n",
       " ('system', 0.6736716628074646),\n",
       " ('computer', 0.6733997464179993),\n",
       " ('statistics', 0.6705882549285889),\n",
       " ('systems', 0.669585645198822),\n",
       " ('applications', 0.6660566926002502),\n",
       " ('numbers', 0.6655443906784058)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "# similar words to the second word \"data\":\n",
    "glove_wiki_vectors.most_similar(four_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b922b29-e591-41a7-b82c-9bcc794d0f10",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medical', 0.821526288986206),\n",
       " ('medicines', 0.7068259716033936),\n",
       " ('health', 0.6808973550796509),\n",
       " ('veterinary', 0.6703415513038635),\n",
       " ('nutrition', 0.6656507253646851),\n",
       " ('studies', 0.6543422341346741),\n",
       " ('physicians', 0.652547299861908),\n",
       " ('dentistry', 0.6510076522827148),\n",
       " ('science', 0.6494085192680359),\n",
       " ('psychiatry', 0.6493012309074402)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "# similar words to the first word \"medicine\":\n",
    "glove_wiki_vectors.most_similar(four_words[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1caa0fc7-900a-4ef5-817f-166cafc8cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sociology', 0.7726682424545288),\n",
       " ('professor', 0.7641003727912903),\n",
       " ('science', 0.7270671725273132),\n",
       " ('philosophy', 0.7264132499694824),\n",
       " ('psychology', 0.7084123492240906),\n",
       " ('mathematics', 0.6896660327911377),\n",
       " ('sciences', 0.677860677242279),\n",
       " ('physics', 0.6771532297134399),\n",
       " ('harvard', 0.6714091300964355),\n",
       " ('institute', 0.6638572216033936)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "# similar words to the first word \"economics\":\n",
    "glove_wiki_vectors.most_similar(four_words[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000272\n",
      "0.546276\n",
      "0.6432488\n",
      "0.7552732\n",
      "0.8798075\n",
      "0.076719455\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "for (word1, word2) in word_pairs:\n",
    "    print(glove_wiki_vectors.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_wiki_vectors do not have representation for the word 'covididiot'\n",
      "glove_wiki_vectors do not have representation for the word 'fomo'\n",
      "glove_wiki_vectors have representation for the word 'frenemies'\n",
      "glove_wiki_vectors do not have representation for the word 'anthropause'\n",
      "glove_wiki_vectors do not have representation for the word 'photobomb'\n",
      "glove_wiki_vectors do not have representation for the word 'selfie'\n",
      "glove_wiki_vectors do not have representation for the word 'pxg'\n",
      "glove_wiki_vectors do not have representation for the word 'pacg'\n",
      "glove_wiki_vectors have representation for the word 'cct'\n",
      "glove_wiki_vectors have representation for the word 'escc'\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "for word in test_words:\n",
    "    if word in glove_wiki_vectors:\n",
    "        print(f\"glove_wiki_vectors have representation for the word '{word}'\")\n",
    "    else:        \n",
    "        print(f\"glove_wiki_vectors do not have representation for the word '{word}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14283238"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.351824"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : manager :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assistant</td>\n",
       "      <td>0.656641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>0.626392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>supervisor</td>\n",
       "      <td>0.593479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owner</td>\n",
       "      <td>0.587832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hired</td>\n",
       "      <td>0.577390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>consultant</td>\n",
       "      <td>0.570659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>director</td>\n",
       "      <td>0.565653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>office</td>\n",
       "      <td>0.564568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>employee</td>\n",
       "      <td>0.561287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ceo</td>\n",
       "      <td>0.556422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0    assistant  0.656641\n",
       "1          job  0.626392\n",
       "2   supervisor  0.593479\n",
       "3        owner  0.587832\n",
       "4        hired  0.577390\n",
       "5   consultant  0.570659\n",
       "6     director  0.565653\n",
       "7       office  0.564568\n",
       "8     employee  0.561287\n",
       "9          ceo  0.556422"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "analogy(\"man\", \"manager\", \"woman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6025b7c9-5ffd-4076-8d44-0440d01efe69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2578922\n",
      "0.40354294\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "print(glove_wiki_vectors.similarity(\"asian\", \"creative\"))\n",
    "print(glove_wiki_vectors.similarity(\"american\", \"creative\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275774\n",
      "0.49818176\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "print(glove_wiki_vectors.similarity(\"christian\", \"extremism\"))\n",
    "print(glove_wiki_vectors.similarity(\"muslim\", \"extremism\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95e306e7-26f3-4367-8935-0be49b86be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48625344\n",
      "0.274084\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "print(glove_wiki_vectors.similarity(\"young\", \"brave\"))\n",
    "print(glove_wiki_vectors.similarity(\"old\", \"brave\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c125c3",
   "metadata": {},
   "source": [
    "1. There are worrisome biases in these embeddings: the embeddings analogize the word pair \"man\" and \"manager\" to the word pair of \"woman\" and \"assistant\", which shows the bias on gender; in my second example, the similarity between \"asian\" and \"creative\" is lower than the one between \"american\" and \"creative\" by roughly 0.15, which shows a bias on different cultures; for the thrid example, the word pair of \"muslim\" and \"extremism\" are more similar than \"christian\" and \"extremism\" are by almost 0.23, showing the bias on religions; in the last example, the word \"brave\" is more similar to \"young\" than to \"old\" by 0.212, showing the age stereotypes in these embeddings trained on Wikipedia.\n",
    "2. When we build a recommendation system using word embeddings with biases, for example, a job recommendation system, it might recommend male for leadership roles based on a stronger assocation of \"man\" with \"manager\" and of \"woman\" with \"assistant\". This would encourage workplace gender inequality, avoid more suitable candidates getting the positions, and also hinder individuals' career development due to their gender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    \\n\\nVery simple.\\n\\n\"X-Soviet Armenian government must pay for their crime of genocide \\n against 2.5 million Muslims by admitting to the crime and making \\n reparations to the Turks and Kurds.\"\\n\\nAfter all, your criminal grandparents exterminated 2.5 million Muslim\\npeople between 1914 and 1920.\\n\\n\\n<C5yyBt.5zo@news.cso.uiuc.edu>\\nhovig@uxa.cso.uiuc.edu (Hovig Heghinian)\\n\\n\\nYou must be a new 'Arromdian'. You are counting on ASALA/SDPA/ARF \\ncrooks and criminals to prove something for you? No wonder you are in \\nsuch a mess. That criminal idiot and 'its' forged/non-existent junk has \\nalready been trashed out by Mutlu, Cosar, Akgun, Uludamar, Akman, Oflazer \\nand hundreds of people. Moreover, ASALA/SDPA/ARF criminals are responsible \\nfor the massacre of the Turkish people that also prevent them from entering \\nTurkiye and TRNC. SDPA has yet to renounce its charter which specifically \\ncalls for the second genocide of the Turkish people. This racist, barbarian \\nand criminal view has been touted by the fascist x-Soviet Armenian government \\nas merely a step on the road to said genocide. \\n\\nNow where shall I begin?\\n\\n#From: ahmet@eecg.toronto.edu (Parlakbilek Ahmet)\\n#Subject: YALANCI, LIAR : DAVIDIAN\\n#Keywords: Davidian, the biggest liar\\n#Message-ID: <1991Jan10.122057.11613@jarvis.csri.toronto.edu>\\n\\nFollowing is the article that Davidian claims that Hasan Mutlu is a liar:\\n\\n\\n\\n\\n\\n\\n[some parts are deleted]\\n\\n\\n\\n\\n\\t\\t\\t\\t- - -\\n\\n\\n\\n\\nReceiving this message, I checked the reference, L.Kuper,\"Genocide...\" and\\nwhat I have found was totally consistent with what Davidian said.The book\\nwas like \"voice of Armenian revolutionists\" and although I read the whole book,\\nI could not find the original quota.\\nBut there was one more thing to check:The original posting of Mutlu.I found \\nthe original article of Mutlu.It is as follows:\\n\\n\\n\\n     ======================================================================\\n    ======\\n\\nQUATO IS THE SAME, REFERENCE IS DIFFERENT !\\n\\nDAVIDIAN LIED AGAIN, AND THIS TIME HE CHANGED THE ORIGINAL POSTING OF MUTLU\\nJUST TO ACCUSE HIM TO BE A LIAR.\\n\\nDavidian, thank you for writing the page number correctly...\\n\\nYou are the biggest liar I have ever seen.This example showed me that tomorrow\\nyou can lie again, and you may try to make me a liar this time.So I decided\\nnot to read your articles and not to write answers to you.I also advise\\nall the netters to do the same.We can not prevent your lies, but at least\\nwe may save time by not dealing with your lies.\\n\\nAnd for the following line:\\n\\nI also return all the insults you wrote about Mutlu to you.\\nI hope you will be drowned in your lies.\\n\\nAhmet PARLAKBILEK\\n\\n#From: vd8@cunixb.cc.columbia.edu (Vedat  Dogan)\\n#Message-ID: <1993Apr8.233029.29094@news.columbia.edu>\\n\\n \\nn>crap posted by Mr. [(*]!\\n \\n o boy!   \\n \\n Please, can you tell us why those quotes are \"crap\"?..because you do not \\n like them!!!...because they really exist...why?\\n \\n As I said in my previous posting, those quotes exactly exist in the source \\n given by Serdar Argic .. \\n  \\n You couldn't reject it...\\n \\n \\n Here we go again..\\n In the book I have, both the front page and the Author's preface give \\n the same year: 1923 and 15 January, 1923, respectively!\\n (Anyone can check it at her/his library,if not, I can send you the copies of\\n pages, please ask by sct) \\n \\n \\nI really don't care what year it was first published(1923 or 1924)\\nWhat I care about is what the book writes about murders, tortures,et..in\\nthe given quotes by Serdar Argic, and your denial of these quotes..and your\\ngroundless accussations, etc. \\n \\n[...]\\n \\n \\n I claim I have a book in my hand published in 1923(first publication)\\n and it exactly has the same quoted info as the book published\\n in 1934(Serdar Argic's Reference) has..You couldn't reject it..but, now you\\n are avoiding the real issues by twisting around..\\n \\n Let's see how you lie!..(from 'non-existing' quotes to re-publication)\\n \\n First you said there was no such a quote in the given reference..You\\n called Serdar Argic a liar!..\\n I said to you, NO, MR.Davidian, there exactly existed such a quote...\\n (I even gave the call number, page numbers..you could't reject it.)\\n \\n And now, you are lying again and talking about \"modified,re-published book\"\\n(without any proof :how, when, where, by whom, etc..)..\\n (by the way, how is it possible to re-publish the book in 1923 if it was\\n  first published in 1924(your claim).I am sure that you have some 'pretty \\n  well suited theories', as usual)\\n \\n And I am ready to send the copies of the necessary pages to anybody who\\n wants to compare the fact and Mr.Davidian's lies...I also give the call number\\n and page numbers again for the library use, which are:  \\n                 949.6 R 198\\n   \\n  and the page numbers to verify the quotes:218 and 215\\n              \\n     \\n \\n \\n Now, are you claiming that there can't be such a reference by saying \"it is\\n not possible...\" ..If not, what is your point?\\n \\n Differences in the number of pages?\\n Mine was published in 1923..Serdar Argic's was in 1934..\\n No need to use the same book size and the same letter \\n charachter in both publications,etc, etc.. does it give you an idea!!\\n \\n The issue was not the number of pages the book has..or the year\\n first published.. \\n And you tried to hide the whole point..\\n the point is that both books have the exactly the same quotes about\\n how moslems are killed, tortured,etc by Armenians..and those quotes given \\n by Serdar Argic exist!! \\n It was the issue, wasn't-it?  \\n \\n you were not able to object it...Does it bother you anyway? \\n \\n You name all these tortures and murders (by Armenians) as a \"crap\"..\\n People who think like you are among the main reasons why the World still\\n has so many \"craps\" in the 1993. \\n \\n Any question?\\n \\n\\n<C5wwqA.9wL@news.cso.uiuc.edu>\\nhovig@uxa.cso.uiuc.edu (Hovig Heghinian)\\n\\n\\nWell, apparently we have another son of Dro 'the Butcher' to contend with. \\nYou should indeed be happy to know that you rekindled a huge discussion on\\ndistortions propagated by several of your contemporaries. If you feel \\nthat you can simply act as an Armenian governmental crony in this forum \\nyou will be sadly mistaken and duly embarrassed. This is not a lecture to \\nanother historical revisionist and a genocide apologist, but a fact.\\n\\nI will dissect article-by-article, paragraph-by-paragraph, line-by-line, \\nlie-by-lie, revision-by-revision, written by those on this net, who plan \\nto 'prove' that the Armenian genocide of 2.5 million Turks and Kurds is \\nnothing less than a classic un-redressed genocide. We are neither in \\nx-Soviet Union, nor in some similar ultra-nationalist fascist dictatorship, \\nthat employs the dictates of Hitler to quell domestic unrest. Also, feel \\nfree to distribute all responses to your nearest ASALA/SDPA/ARF terrorists,\\nthe Armenian pseudo-scholars, or to those affiliated with the Armenian\\ncriminal organizations.\\n\\nArmenian government got away with the genocide of 2.5 million Turkish men,\\nwomen and children and is enjoying the fruits of that genocide. You, and \\nthose like you, will not get away with the genocide's cover-up.\\n\\nNot a chance.\\n\\nSerdar Argic\n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \\nNo no no!!!  It's a squid!  Keep the tradition alive!  (Kinda like the\\nfish at UNH games....)\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "# take a small subset and see the current text:\n",
    "subset_df = df.head(100)  # select the first 100 rows of data\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(subset_df.iloc[6:8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef72f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re   # regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba75d96d-01dc-4b3b-b222-67c63b5fd0c5",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "# reference: Lecture 18\n",
    "\n",
    "def preprocess(doc, irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"]):\n",
    "    clean_text = []\n",
    "    \n",
    "    text = doc.text\n",
    "    # getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "    # replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "    # reference: https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', 'EMAIL', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', 'NUM', text) \n",
    "    text = re.sub(r'[\\n/\\\\-]', ' ', text)\n",
    "    text = text.strip()\n",
    "    # sentence segmentation and tokenization  \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Filter tokens based on various criteria:\n",
    "    # getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "    # excluding stopwords and punctuation \n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and token.is_punct == False  # punctuation\n",
    "            and token.is_space == False\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "            ):  # Check if the POS is in the acceptable POS tags\n",
    "            # lemmatization:\n",
    "            lemma = token.lemma_  \n",
    "            clean_text.append(lemma.lower())\n",
    "\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaa8f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunxi\\AppData\\Local\\Temp\\ipykernel_20612\\243745135.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(subset_df[\"text\"])]\n"
     ]
    }
   ],
   "source": [
    "# try the preprocess function on the subset\n",
    "subset_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(subset_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "366b2372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6    simple x soviet armenian government pay crime genocide num.num million muslims admit crime make reparation turks kurds criminal grandparent exterminate num.num million muslim people num num email email hovig heghinian new arromdian count asala sdpa arf crook criminal prove wonder mess criminal idiot forge non existent junk trash mutlu cosar akgun uludamar akman oflazer hundred people asala sdpa arf criminal responsible massacre turkish people prevent enter turkiye trnc sdpa renounce charter call second genocide turkish people racist barbarian criminal view tout fascist x soviet armenian government step road say genocide shall begin email parlakbilek ahmet subject yalanci liar davidian keywords davidian big liar message id email follow article davidian claim hasan mutlu liar part delete receive message check reference l.kuper,\"genocide find consistent davidian say book voice armenian revolutionist read book find original quota thing check original posting mutlu find original article mutlu follow = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = quato reference different davidian lie time change original posting mutlu accuse liar davidian thank write page number big liar see example show tomorrow lie try liar time decide read article write answer advise netter prevent lie save time deal lie follow line return insult write mutlu hope drown lie ahmet parlakbilek email vedat dogan message id email > crap post mr. o boy tell quote crap\"? them!!! exist say previous posting quote exist source give serdar argic reject book page author preface year num num january num check library send copy page ask sct care year published(num num care book write murder torture et give quote serdar argic denial quote groundless accussation etc claim book hand publish num(first publication quote info book publish num(serdar argic reference reject avoid real issue twist let lie! non exist quote publication say quote give reference call serdar argic liar say mr.davidian exist quote give number page number could't reject lie talk modify publish book proof etc way possible publish book num publish num(your claim).i sure suited theory usual ready send copy necessary page want compare fact mr. davidian lie number page number library use num.num r num page number verify quote num num claim reference say possible point difference number page publish num serdar argic num need use book size letter charachter publication etc etc idea issue number page book year publish try hide point point book quote moslem kill torture etc armenians quote give serdar argic exist issue able object bother torture murder armenians crap people think main reason world crap num question email email hovig heghinian son dro butcher contend happy know rekindle huge discussion distortion propagate contemporary feel act armenian governmental crony forum mistaken embarrassed lecture historical revisionist genocide apologist fact dissect article article paragraph paragraph line line lie lie revision revision write net plan prove armenian genocide num.num million turks kurds classic un redress genocide x soviet union similar ultra nationalist fascist dictatorship employ dictate hitler quell domestic unrest feel free distribute response near asala sdpa arf terrorist armenian pseudo scholar affiliate armenian criminal organization armenian government get genocide num.num million turkish man woman child enjoy fruit genocide genocide cover chance serdar argic\n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                squid tradition alive fish unh game\n",
      "Name: text_pp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check on the first two rows:\n",
    "print(subset_df.iloc[6:8][\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0782a094-bd30-4e80-9678-8f1197ca6a28",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "# df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df[\"text\"])]\n",
    "# the code above crahsed my kernel so I devide the df into 3 parts and after running each of them i restart the kernel\n",
    "# therefore, i commented out the followed preprocessing code and save the preprocessed results into three separate csv files\n",
    "# finally i merged them into one final df, which will be used later in the model training part.\n",
    "n = len(df)\n",
    "df1 = df.iloc[:n//3]  \n",
    "df2 = df.iloc[n//3:2*n//3]  \n",
    "df3 = df.iloc[2*n//3:]  \n",
    "df3_1 = df3.iloc[:n//3]  # actually i failed to further split the third part, but the final row number of df1,df2 and df3_1 equals that of df (please see my following code to ensure)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e05251d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df1[\"text\"])]\n",
    "# df1.to_csv(\"df1_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecc2f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df2[\"text\"])]\n",
    "# df2.to_csv(\"df2_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3256036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3_1[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df3_1[\"text\"])]\n",
    "# df3_1.to_csv(\"df3_1_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c5a2b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563\n",
      "4563\n"
     ]
    }
   ],
   "source": [
    "print(n)\n",
    "print(len(df1)+len(df2)+len(df3_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc8f51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"df1_preprocessed.csv\")\n",
    "df2 = pd.read_csv(\"df2_preprocessed.csv\")\n",
    "df3 = pd.read_csv(\"df3_1_preprocessed.csv\")\n",
    "\n",
    "# combine the three files\n",
    "final_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# save the final preprocessing result\n",
    "final_df.to_csv(\"final_preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Magazine but wonder if there is one on\\ndisk instead of on paper. Having it on disk would save me retyping\\nillustrations, etc into a word processor. It's just cut and paste.\\n   If there are other good Christian magazines like Leadership on disk media,\\nI'd appreciate any info.</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "2  \\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                               The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?   \n",
       "4                                                                                                                                                                                          \\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Hi,\\n   I'd like to subscribe to Leadership Magazine but wonder if there is one on\\ndisk instead of on paper. Having it on disk would save me retyping\\nillustrations, etc into a word processor. It's just cut and paste.\\n   If there are other good Christian magazines like Leadership on disk media,\\nI'd appreciate any info.   \n",
       "\n",
       "   target             target_name  \n",
       "2       1           comp.graphics  \n",
       "3       2          comp.windows.x  \n",
       "4       7   talk.politics.mideast  \n",
       "5       5  soc.religion.christian  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52086045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have newsgroup aspect graphic programming propose brian reply original posting suggest loose structure comp.graphics.programmer reason make group possible reason spilt posting day take time browse posting find interested understand agree want aspect graphic meeting agree extension news forum exchange idea help help think difficult achive different thing meeting good evening netters|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create window w1 child root event_mask = buttonpressmask|keypressmask w2 child w1 do_not_propagate_mask buttonpressmask|keypressmask keypress event w2 discard buttonpress event fall w1 subwindow set w2 fyi xnew olvwm wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ataman tell journalist turkey close air space flight armenia prevent humanitarian aid reach republic overland turkish territory uncivilized people exhibit sign compassion allow humanitarian aid reach civilian population nazis turkey pronounce hypocrite choose continue condemnation serbians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Magazine but wonder if there is one on\\ndisk instead of on paper. Having it on disk would save me retyping\\nillustrations, etc into a word processor. It's just cut and paste.\\n   If there are other good Christian magazines like Leadership on disk media,\\nI'd appreciate any info.</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>hi like subscribe leadership magazine wonder disk paper have disk save retype illustration etc word processor cut paste good christian magazine leadership disk medium appreciate info</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "2  \\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                               The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?   \n",
       "4                                                                                                                                                                                          \\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Hi,\\n   I'd like to subscribe to Leadership Magazine but wonder if there is one on\\ndisk instead of on paper. Having it on disk would save me retyping\\nillustrations, etc into a word processor. It's just cut and paste.\\n   If there are other good Christian magazines like Leadership on disk media,\\nI'd appreciate any info.   \n",
       "\n",
       "   target             target_name  \\\n",
       "2       1           comp.graphics   \n",
       "3       2          comp.windows.x   \n",
       "4       7   talk.politics.mideast   \n",
       "5       5  soc.religion.christian   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                         text_pp  \n",
       "2  actuallay hand support idea have newsgroup aspect graphic programming propose brian reply original posting suggest loose structure comp.graphics.programmer reason make group possible reason spilt posting day take time browse posting find interested understand agree want aspect graphic meeting agree extension news forum exchange idea help help think difficult achive different thing meeting good evening netters|  \n",
       "3                                                                                                                                                              follow problem bug appreciate help create window w1 child root event_mask = buttonpressmask|keypressmask w2 child w1 do_not_propagate_mask buttonpressmask|keypressmask keypress event w2 discard buttonpress event fall w1 subwindow set w2 fyi xnew olvwm wrong  \n",
       "4                                                                                  late upi foreign ministry spokesman ferhat ataman tell journalist turkey close air space flight armenia prevent humanitarian aid reach republic overland turkish territory uncivilized people exhibit sign compassion allow humanitarian aid reach civilian population nazis turkey pronounce hypocrite choose continue condemnation serbians  \n",
       "5                                                                                                                                                                                                                                         hi like subscribe leadership magazine wonder disk paper have disk save retype illustration etc word processor cut paste good christian magazine leadership disk medium appreciate info  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a06bf7",
   "metadata": {},
   "source": [
    "- the input is not a text object, so first I use .text() to convert it to text.\n",
    "- removing the non-informative characters including slashes, new-line characters, hyphens, and other white spaces using .re.sub(r'[\\n/\\\\-]', ' ', text) and .strip();\n",
    "- using functions in regex to replace urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\".\n",
    "- using doc = nlp(text), i get a structured text object, which allows us to extract information such as tokenization results, part-of-speech (POS) tagging more easily. Here we realized sentence segmentation and tokenization with it.\n",
    "- filter tokens: removing stopwords, punctuation and space, also getting rid of the POS which are not going to be helpful in topic modeling\n",
    "- lemmatization using .lemma_ to get the original word format.\n",
    "- using .lower() to get the lowercase of the words.\n",
    "- add the result words into the final clean text list, and convert them to a string in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a649",
   "metadata": {},
   "source": [
    "below, I build a LDA models on the preprocessed data using LatentDirichletAllocation, and I try with three values for n_components. Comparing these three results, I think 4 is the most reasonable one for the the number of topics (programming, religion, politics, and sports for topic 0, 1, 2, 3, respectively). For other higher values, some topics are seemingly good to be merged with other group, such as topic 2 and 3 (both look like a topic of sports or game) for n=6, and topic 0 and 5 (programming), topic 2 and 3 for n=8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c503163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of missing values\n",
    "final_df[\"text_pp\"] = final_df[\"text_pp\"].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce18a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(final_df[\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f25ed7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "num           god           num           num           \n",
      "file          people        people        email         \n",
      "email         num           say           game          \n",
      "program       think         year          play          \n",
      "use           know          think         team          \n",
      "window        believe       gun           season        \n",
      "image         jesus         know          new           \n",
      "entry         say           time          period        \n",
      "include       thing         come          la            \n",
      "server        time          good          goal          \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "num           god           say           num           num           \n",
      "entry         people        think         game          gun           \n",
      "file          know          year          play          people        \n",
      "armenian      think         people        team          law           \n",
      "output        believe       know          season        jews          \n",
      "program       jesus         come          email         right         \n",
      "armenians     thing         num           period        weapon        \n",
      "line          say           good          goal          state         \n",
      "armenia       question      time          la            know          \n",
      "rule          church        team          new           firearm       \n",
      "\n",
      "\n",
      "topic 5       \n",
      "--------      \n",
      "num           \n",
      "email         \n",
      "file          \n",
      "image         \n",
      "use           \n",
      "window        \n",
      "program       \n",
      "available     \n",
      "run           \n",
      "server        \n",
      "\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "num           god           think         num           num           \n",
      "entry         believe       say           game          gun           \n",
      "file          people        year          play          people        \n",
      "output        jesus         know          team          law           \n",
      "program       know          good          season        say           \n",
      "line          think         come          period        weapon        \n",
      "build         church        game          goal          right         \n",
      "rule          question      team          la            know          \n",
      "section       thing         time          new           firearm       \n",
      "char          say           people        year          think         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      \n",
      "num           num           num           \n",
      "email         armenian      israel        \n",
      "file          turkish       jews          \n",
      "image         armenians     israeli       \n",
      "window        turkey        people        \n",
      "use           email         jewish        \n",
      "program       people        state         \n",
      "available     turks         time          \n",
      "run           armenia       war           \n",
      "server        greek         right         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_components = [4, 6, 8]   # Experiment with different numbers of topics\n",
    "lda_models = {}\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "\n",
    "for n in n_components:\n",
    "    lda = LatentDirichletAllocation(n_components=n, random_state=42)\n",
    "    lda.fit_transform(X)\n",
    "    sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "    \n",
    "    mglearn.tools.print_topics(\n",
    "        topics=range(n),\n",
    "        feature_names=feature_names,\n",
    "        sorting=sorting,\n",
    "        topics_per_chunk=5,\n",
    "        n_words=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference: Lecture 18\n",
    "\n",
    "n_topics = 4\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=42\n",
    ")\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1afbdb-b419-4d17-a7fa-0dfd9e618f15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "967136f3-5746-4568-8d2d-d48c4d3be36c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "num           god           num           num           \n",
      "file          people        people        email         \n",
      "email         num           say           game          \n",
      "program       think         year          play          \n",
      "use           know          think         team          \n",
      "window        believe       gun           season        \n",
      "image         jesus         know          new           \n",
      "entry         say           time          period        \n",
      "include       thing         come          la            \n",
      "server        time          good          goal          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "document_topics = lda.fit_transform(X)\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(4),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16929025",
   "metadata": {},
   "source": [
    "topic 0: computer science, topic 1: religion, topic 2: politics, topic 3: sports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18caef39-4161-438b-90f2-99337f39dcd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f09783a-5644-46ac-bb2f-d60947a13d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: topic = 2\n",
      "Document 2: topic = 0\n",
      "Document 3: topic = 1\n",
      "Document 4: topic = 0\n",
      "Document 5: topic = 2\n"
     ]
    }
   ],
   "source": [
    "first_five_topics = document_topics[:5]\n",
    "for i, topic_probs in enumerate(first_five_topics):\n",
    "    most_probable_topic = topic_probs.argmax()  # get the topic with the highest probability\n",
    "    print(f\"Document {i+1}: topic = {most_probable_topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df3f2bb",
   "metadata": {},
   "source": [
    "According to:\n",
    "\n",
    "- topic 0: computer science,\n",
    "- topic 1: religion,\n",
    "- topic 2: politics,\n",
    "- topic 3: sports.\n",
    "---\n",
    "- Document 1: topic = politics\n",
    "- Document 2: topic = computer science\n",
    "- Document 3: topic = religion\n",
    "- Document 4: topic = computer science\n",
    "- Document 5: topic = politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c7dba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       talk.politics.guns\n",
       "1            comp.graphics\n",
       "2            comp.graphics\n",
       "3           comp.windows.x\n",
       "4    talk.politics.mideast\n",
       "Name: target_name, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target_name\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92fe65d",
   "metadata": {},
   "source": [
    "Here we can see document 1, 2, 4, 5 all correspond to the correct topics. There might not be very detailed information like either guns or mideast in politics, but the big theme are nearly all right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing? Briefly explain.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f9fa2",
   "metadata": {},
   "source": [
    "1. content-based filtering filters out and then recommends the items to users based on the features of the items, like the movie themes for movies, and some demographic and preference information on users, such as age and education. The items similar to the items with higher ratings will be recommended to the users.\n",
    "2. first negative consequence of recommender systems is the effect of filter bubbles, which limits a user's exposure to diverse viewpoints and information and narrows their thinkings. The second negative consequence can be reinforcing extreme views: Recommendation systems often optimize for engagement (likes, clicks, shares), which may recommend increasingly extreme content since such content tends to capture attention more effectively.\n",
    "3. Transfer learning leverages knowledge learned from one task or domain and apply it to another, typically related, task or domain. It involves training a model on a large, diverse dataset and then fine-tuning it on a specific target corpus (domain-specific dataset), such as healthcare or social media. This can improve performance and reduce training costs since general linguistic patterns have been learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
